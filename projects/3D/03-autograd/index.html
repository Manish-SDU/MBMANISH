<!DOCTYPE html><html lang="en">
<!-- Mirrored from www.python-engineer.com/courses/pytorchbeginner/03-autograd/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 04 Sep 2022 12:02:05 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset="UTF-8"/><meta name="og:site_name" content="Python Engineer"/><link rel="canonical" href="https://python-engineer.com/courses/pytorchbeginner/03-autograd"/><meta name="twitter:url" content="https://python-engineer.com/courses/pytorchbeginner/03-autograd"/><meta name="og:url" content="https://python-engineer.com/courses/pytorchbeginner/03-autograd"/><title>Autograd - PyTorch Beginner 03 | Python Engineer</title><meta name="twitter:title" content="Autograd - PyTorch Beginner 03 | Python Engineer"/><meta name="og:title" content="Autograd - PyTorch Beginner 03 | Python Engineer"/><meta name="description" content="In this part we learn how to calculate gradients using the autograd package in PyTorch."/><meta name="twitter:description" content="In this part we learn how to calculate gradients using the autograd package in PyTorch."/><meta name="og:description" content="In this part we learn how to calculate gradients using the autograd package in PyTorch."/><meta name="twitter:card" content="summary"/><link rel="stylesheet" href="../../../styles.css" type="text/css"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="shortcut icon" href="../../../images/favicon.png" type="image/png"/><link rel="alternate" href="../../../feed.rss" type="application/rss+xml" title="Subscribe to Python Engineer"/><meta name="twitter:image" content="../../../images/icon.png"/><meta name="og:image" content="../../../images/icon.png"/><script async defer data-domain="python-engineer.com" src="../../../../plausible.io/js/plausible.js"></script></head><body class="item"><header><div class="header-promotion"><p><div class="wrapper"><p><a href="https://pythonengineer.pallet.com/" target="_blank">üêç Find Python and ML Jobs! üêç</a></p></div></p></div><div class="wrapper"><nav><ul><li><a href="../../../posts/index.html">POSTS</a></li><li class="selected"><a href="../../index.html">COURSES</a></li><li><a href="../../../about/index.html">ABOUT</a></li></ul></nav><a href="../../../index.html"><div class="hero-container"><img class="header-image-small" src="../../../images/pat_face_bg.webp" alt="Patrick Loeber"/><div class="hero-text-container justify-center"><p>Become a</p><h1 class="colorized-text ">Python Engineer</h1><p class="mobile-hide">Come and learn about</p><ul class="mobile-hide"><li><a class="variant-a" href="../../../tags/python/index.html">Python</a></li><li id="regular-li">and</li><li><a class="variant-b" href="../../../tags/machine-learning/index.html">Machine Learning</a></li></ul></div></div></a></div></header><article class="page wrapper post-page"><a class="back" href="../index.html">Back to course overview</a><h1 class="blog-header">Autograd - PyTorch Beginner 03</h1><div class="metadata"><ul class="tags"><li class="variant-c"><a href="../../../tags/pytorch/index.html">PyTorch</a></li><li class="variant-c"><a href="../../../tags/deep-learning/index.html">Deep Learning</a></li></ul><span class="post-date">25 Dec 2019, by </span><a class="author-link-post" href="../../../authors/patrick/index.html">Patrick Loeber</a></div><div class="video-player"><iframe frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="true" src="https://www.youtube-nocookie.com/embed/DbeIqrwb_dE"></iframe></div><div class="content page-content m-top"><p>Learn all the basics you need to get started with this deep learning framework! In this part we learn how to calculate gradients using the autograd package in PyTorch. This tutorial contains the following topics:</p><ul><li><code>requires_grad</code> attribute for Tensors</li><li>Computational graph</li><li>Backpropagation (brief explanation)</li><li>How to stop autograd from tracking history</li><li>How to zero (empty) gradients</li></ul><p>All code from this course can be found on <a href="https://github.com/python-engineer/pytorchTutorial">GitHub</a>.</p><h2>The Autograd package</h2><p>The autograd package provides automatic differentiation for all operations on Tensors. To tell PyTorch that we want the gradient, we have to set <code>requires_grad=True</code>. With this attribute set, all operations on the tensor are tracked in the computational graph.</p><pre><code><div class="highlight"><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="c1"># requires_grad = True -&gt; tracks all operations on the tensor. </span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>

<span class="c1"># y was created as a result of an operation, so it has a grad_fn attribute.</span>
<span class="c1"># grad_fn: references a Function that has created the Tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># created by the user -&gt; grad_fn is None</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="c1"># Do more operations on y</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</div></code></pre><h2>Let's compute the gradients with backpropagation</h2><p>When we finish our computation we can simply call <code>.backward()</code> and have all the gradients computed automatically. The gradient for this tensor will be accumulated into <code>.grad</code> attribute. It is the partial derivate of the function w.r.t. the tensor.</p><pre><code><div class="highlight"><span></span><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># dz/dx</span>
</div></code></pre><p>Generally speaking, <code>torch.autograd</code> is an engine for computing vector-Jacobian product. It computes partial derivates while applying the chain rule.</p><pre><code><div class="highlight"><span></span><span class="c1"># Model with non-scalar output:</span>
<span class="c1"># If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() </span>
<span class="c1"># specify a gradient argument that is a tensor of matching shape.</span>
<span class="c1"># needed for vector-Jacobian product</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</div></code></pre><h2>Stop a tensor from tracking history</h2><p>For example during our training loop when we want to update our weights then this update operation should not be part of the gradient computation. We have 3 options to stop gradient calculations:</p><ul><li><code>x.requires_grad_(False)</code></li><li><code>x.detach()</code></li><li>wrap in <code>with torch.no_grad():</code></li></ul><h3><code>.requires_grad_(...)</code> changes an existing flag in-place:</h3><pre><code><div class="highlight"><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</div></code></pre><h3><code>.detach()</code>: get a new Tensor with the same content but no gradient computation:</h3><pre><code><div class="highlight"><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</div></code></pre><h3>wrap in <code>with torch.no_grad()</code>:</h3><pre><code><div class="highlight"><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</div></code></pre><h2>Empty gradients!</h2><p><code>backward()</code> <strong>accumulates the gradient</strong> for this tensor into the <code>.grad</code> attribute. We need to be careful during optimization !!!<br>-&gt; Use <code>.zero_()</code> to empty the gradients before a new optimization step!</p><pre><code><div class="highlight"><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># just a dummy example</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">model_output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

    <span class="c1"># optimize model, i.e. adjust weights...</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">weights</span> <span class="o">-=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span>

    <span class="c1"># this is important! It affects the final weights &amp; output</span>
    <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</div></code></pre><h3>Optimizer has <code>zero_grad()</code> method</h3><p>(We will learn about optimizer in tutorial #6)</p><pre><code><div class="highlight"><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">weights</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># During training:</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</div></code></pre></div><ul class="actions"><li><a class="youtube-button" href="https://www.youtube.com/channel/UCbXgNpp0jedKWcQiULLbDTA?sub_confirmation=1" target="_blank" rel="nofollow">Subscribe</a></li><li><a class="twitter-button" href="https://twitter.com/intent/tweet?via=python_engineer&amp;text=Autograd%20-%20PyTorch%20Beginner%2003&amp;url=https://python-engineer.com/courses/pytorchbeginner/03-autograd" rel="nofollow" target="_blank">Share</a></li></ul><div class="affiliate-container"><div><h2>FREE VS Code / PyCharm Extensions I Use</h2><p>‚úÖ Write cleaner code with Sourcery, instant refactoring suggestions: <a href="https://sourcery.ai/?utm_source=youtube&amp;utm_campaign=pythonengineer" target="_blank">Link *</a></p><p class="affiliate-note">* This is an affiliate link. By clicking on it you will not have any additional costs, instead you will support me and my project. Thank you! üôè</p></div></div><a class="sponsor-container" href="../../../newsletter/index.html" rel="nofollow"><img class="main" src="../../../images/numpyhandbook.webp" alt="newsletter"/><div><h2>FREE NumPy Handbook</h2><p>Learn NumPy with this eBook! It covers code examples for all essential functions. Get it for free together with monthly Python tips and news.</p><button class="pill-button">I Want This</button></div></a><div><div><a class="section-header pad-top" href="../../index.html"><h2>Check out my Courses</h2></a><ul class="item-list grid"><li><article class="post-card"><a href="../../tensorflowbeginner/index.html"><img class="post-thumb" alt="Thumbnail image of the post." src="../../../images/titles/tfcourse.webp"/></a><ul class="tags"><li class="variant-a"><a href="../../../tags/python/index.html">Python</a></li><li class="variant-f"><a href="../../../tags/tensorflow/index.html">TensorFlow</a></li><li class="variant-c"><a href="../../../tags/deep-learning/index.html">Deep Learning</a></li></ul><a class="post-link" href="../../tensorflowbeginner/index.html"><h1 class="description">TensorFlow 2 Beginner</h1><p>Learn all the necessary basics to get started with TensorFlow 2 and Keras.</p></a></article></li><li><article class="post-card"><a href="../index.html"><img class="post-thumb" alt="Thumbnail image of the post." src="../../../images/titles/pytorchcourse.webp"/></a><ul class="tags"><li class="variant-a"><a href="../../../tags/python/index.html">Python</a></li><li class="variant-c"><a href="../../../tags/pytorch/index.html">PyTorch</a></li><li class="variant-c"><a href="../../../tags/deep-learning/index.html">Deep Learning</a></li></ul><a class="post-link" href="../index.html"><h1 class="description">PyTorch Beginner</h1><p>Learn all the necessary basics to get started with this deep learning framework.</p></a></article></li><li><article class="post-card"><a href="../../mlfromscratch/index.html"><img class="post-thumb" alt="Thumbnail image of the post." src="../../../images/titles/mlfromscratch.webp"/></a><ul class="tags"><li class="variant-a"><a href="../../../tags/python/index.html">Python</a></li><li class="variant-b"><a href="../../../tags/machine-learning/index.html">Machine Learning</a></li><li class="variant-d"><a href="../../../tags/numpy/index.html">numpy</a></li></ul><a class="post-link" href="../../mlfromscratch/index.html"><h1 class="description">ML From Scratch</h1><p>Implement popular Machine Learning algorithms from scratch using only built-in Python modules and numpy.</p></a></article></li><li><article class="post-card"><a href="../../advancedpython/index.html"><img class="post-thumb" alt="Thumbnail image of the post." src="../../../images/titles/advancedpythoncourse.webp"/></a><ul class="tags"><li class="variant-a"><a href="../../../tags/python/index.html">Python</a></li></ul><a class="post-link" href="../../advancedpython/index.html"><h1 class="description">Advanced Python</h1><p>Advanced Python Tutorials. It covers topics like collections, decorators, generators, multithreading, logging, and much more.</p></a></article></li></ul></div></div></article><footer><div class="wrapper patreon-footer"><h1>Patreon</h1><p>Become a Patron and get exclusive content! Get access to ML From Scratch notebooks, join a private Discord channel, get priority response, and more!</p><div class="gold-patrons"><h3>Special thanks to my Gold Patreons:</h3><p>Tonja J R</p><p>Daniel And Andy</p><p>Sergei</p></div><p>And of course thanks to every other member! I really appreciate the support!</p><a href="https://www.patreon.com/patrickloeber" target="_blank">Find Out More</a></div><div class="wrapper"><p>Copyright ¬© Python Engineer 2022.</p><p>Generated using <a href="https://github.com/johnsundell/publish" target="_blank">Publish</a>.</p><ul class="footer-links"><li><a class="youtube-button youtube-icon" href="https://www.youtube.com/channel/UCbXgNpp0jedKWcQiULLbDTA" target="_blank">YouTube</a></li><li><a class="social-button twitter-button" href="https://twitter.com/python_engineer" target="_blank">Twitter</a></li><li><a class="social-button github-button" href="https://github.com/python-engineer" target="_blank">GitHub</a></li><li><a class="social-button discord-button" href="https://discord.gg/FHMg9tKFSN" target="_blank">Discord</a></li><li><a class="footer-button" href="../../../feed.rss">RSS</a></li><li><a class="footer-button" href="../../../about/index.html">Contact</a></li><li><a class="footer-button" href="../../../legal-notice/index.html">Legal Notice</a></li><li><a class="footer-button" href="../../../privacy-policy/index.html">Privacy Policy</a></li></ul></div></footer></body>
<!-- Mirrored from www.python-engineer.com/courses/pytorchbeginner/03-autograd/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 04 Sep 2022 12:02:05 GMT -->
</html>